#!/usr/bin/env python3
"""
Hive-Reflex æ¨¡å‹è‡ªåŠ¨åŒ–éƒ¨ç½²å·¥å…·é“¾
åŠŸèƒ½ï¼šåˆ‡ç‰‡ â†’ é‡åŒ– â†’ æ‰“åŒ…æˆ Flash .bin æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•:
    # ä» PyTorch æ¨¡å‹éƒ¨ç½²
    python model_to_flash.py --input model.pth --output firmware.bin
    
    # ä» ONNX æ¨¡å‹éƒ¨ç½²
    python model_to_flash.py --input model.onnx --output firmware.bin
    
    # è‡ªåŠ¨é‡åŒ–å’Œåˆ‡ç‰‡å¤§æ¨¡å‹
    python model_to_flash.py --input large_model.onnx --output firmware.bin --auto-slice

@file model_to_flash.py
@version 2.1.0
"""

import torch
import torch.nn as nn
import numpy as np
import onnx
from onnx import numpy_helper
import struct
import os
import argparse
import json
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)


# ============================================================================
# é…ç½®å¸¸é‡
# ============================================================================

FLASH_PAGE_SIZE = 4096  # Flash é¡µå¤§å°ï¼ˆå­—èŠ‚ï¼‰
CIM_SRAM_SIZE = 512 * 1024  # 512 KB
MAX_LAYER_SIZE = 256 * 1024  # å•å±‚æœ€å¤§å°ºå¯¸

FIRMWARE_MAGIC = b'HRF2'  # Hive-Reflex Firmware v2
FIRMWARE_VERSION = 0x0210  #  2.1.0


# ============================================================================
# æ¨¡å‹åŠ è½½
# ============================================================================

def load_pytorch_model(model_path: str) -> nn.Module:
    """åŠ è½½ PyTorch æ¨¡å‹"""
    logger.info(f"ğŸ“¦ åŠ è½½ PyTorch æ¨¡å‹: {model_path}")
    
    if model_path.endswith('.pth') or model_path.endswith('.pt'):
        model = torch.load(model_path, map_location='cpu')
        if isinstance(model, dict):  # state_dict
            raise ValueError("è¯·æä¾›å®Œæ•´æ¨¡å‹ï¼Œè€Œé state_dict")
        return model
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ PyTorch æ–‡ä»¶æ ¼å¼: {model_path}")


def load_onnx_model(model_path: str) -> onnx.ModelProto:
    """åŠ è½½ ONNX æ¨¡å‹"""
    logger.info(f"ğŸ“¦ åŠ è½½ ONNX æ¨¡å‹: {model_path}")
    
    model = onnx.load(model_path)
    onnx.checker.check_model(model)
    
    return model


def extract_onnx_weights(model: onnx.ModelProto) -> Dict[str, np.ndarray]:
    """ä» ONNX æ¨¡å‹æå–æƒé‡"""
    weights = {}
    
    for initializer in model.graph.initializer:
        weights[initializer.name] = numpy_helper.to_array(initializer)
    
    logger.info(f"  æå– {len(weights)} ä¸ªæƒé‡å¼ é‡")
    
    return weights


# ============================================================================
# é‡åŒ–
# ============================================================================

def quantize_weights_int8(weights: np.ndarray, symmetric: bool = True) -> Tuple[np.ndarray, float]:
    """
    é‡åŒ–æƒé‡åˆ° INT8
    
    Returns:
        quantized: INT8 é‡åŒ–åçš„æƒé‡
        scale: é‡åŒ–æ¯”ä¾‹å› å­
    """
    if symmetric:
        # å¯¹ç§°é‡åŒ– [-127, 127]
        abs_max = np.abs(weights).max()
        scale = abs_max / 127.0 if abs_max > 0 else 1.0
        quantized = np.clip(np.round(weights / scale), -127, 127).astype(np.int8)
    else:
        # éå¯¹ç§°é‡åŒ– [0, 255]
        w_min = weights.min()
        w_max = weights.max()
        scale = (w_max - w_min) / 255.0 if w_max > w_min else 1.0
        zero_point = int(-w_min / scale)
        quantized = np.clip(np.round(weights / scale + zero_point), 0, 255).astype(np.uint8)
        # è½¬å›æœ‰ç¬¦å·è¡¨ç¤º
        quantized = (quantized.astype(np.int16) - zero_point).astype(np.int8)
    
    return quantized, scale


def quantize_model(weights_dict: Dict[str, np.ndarray]) -> Dict[str, Dict]:
    """
    é‡åŒ–æ•´ä¸ªæ¨¡å‹çš„æƒé‡
    
    Returns:
        quantized_dict: å­—å…¸ï¼ŒåŒ…å«é‡åŒ–æƒé‡å’Œ scale
    """
    logger.info(f"ğŸ”¢ é‡åŒ–æ¨¡å‹æƒé‡åˆ° INT8...")
    
    quantized_dict = {}
    total_original_size = 0
    total_quantized_size = 0
    
    for name, weights in weights_dict.items():
        original_size = weights.nbytes
        
        quantized, scale = quantize_weights_int8(weights, symmetric=True)
        quantized_size = quantized.nbytes
        
        quantized_dict[name] = {
            'weights': quantized,
            'scale': scale,
            'shape': weights.shape,
            'dtype': 'int8'
        }
        
        total_original_size += original_size
        total_quantized_size += quantized_size
    
    compression_ratio = total_original_size / total_quantized_size if total_quantized_size > 0 else 1
    
    logger.info(f"  åŸå§‹å¤§å°: {total_original_size/1024:.1f} KB")
    logger.info(f"  é‡åŒ–å: {total_quantized_size/1024:.1f} KB")
    logger.info(f"  å‹ç¼©æ¯”: {compression_ratio:.2f}x")
    
    return quantized_dict


# ============================================================================
# æ¨¡å‹åˆ‡ç‰‡
# ============================================================================

def slice_model_layers(quantized_dict: Dict[str, Dict], max_layer_size: int = MAX_LAYER_SIZE) -> List[Dict]:
    """
    å°†æ¨¡å‹åˆ‡ç‰‡ä¸ºé€‚åˆ CIM SRAM çš„å±‚ç»„
    
    Args:
        quantized_dict: é‡åŒ–åçš„æƒé‡å­—å…¸
        max_layer_size: å•ä¸ªåˆ‡ç‰‡çš„æœ€å¤§å°ºå¯¸ï¼ˆå­—èŠ‚ï¼‰
    
    Returns:
        slices: åˆ‡ç‰‡åˆ—è¡¨ï¼Œæ¯ä¸ªåˆ‡ç‰‡åŒ…å«å¤šä¸ªå±‚
    """
    logger.info(f"âœ‚ï¸  åˆ‡ç‰‡æ¨¡å‹ (æœ€å¤§åˆ‡ç‰‡: {max_layer_size/1024:.0f} KB)...")
    
    slices = []
    current_slice = {'layers': {}, 'size': 0}
    
    for name, layer_data in quantized_dict.items():
        layer_size = layer_data['weights'].nbytes + 4 + 4  # weights + scale + shape
        
        # å¦‚æœå•å±‚è¶…è¿‡æœ€å¤§å°ºå¯¸ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ‡åˆ†æƒé‡
        if layer_size > max_layer_size:
            logger.warning(f"  å±‚ {name} ({layer_size/1024:.1f} KB) è¶…è¿‡æœ€å¤§å°ºå¯¸ï¼Œå°†è¿›è¡Œæƒé‡åˆ‡åˆ†")
            # è¿™é‡Œå®ç°æƒé‡çº§åˆ«çš„åˆ‡åˆ†ï¼ˆç®€åŒ–ç‰ˆï¼‰
            # å®é™…éœ€è¦è€ƒè™‘è®¡ç®—å›¾çš„ä¾èµ–å…³ç³»
            sub_slices = _split_large_layer(name, layer_data, max_layer_size)
            for sub_slice in sub_slices:
                slices.append(sub_slice)
            continue
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°åˆ‡ç‰‡
        if current_slice['size'] + layer_size > max_layer_size:
            if current_slice['layers']:  # å½“å‰åˆ‡ç‰‡éç©º
                slices.append(current_slice)
            current_slice = {'layers': {}, 'size': 0}
        
        # æ·»åŠ å±‚åˆ°å½“å‰åˆ‡ç‰‡
        current_slice['layers'][name] = layer_data
        current_slice['size'] += layer_size
    
    # æ·»åŠ æœ€åä¸€ä¸ªåˆ‡ç‰‡
    if current_slice['layers']:
        slices.append(current_slice)
    
    logger.info(f"  ç”Ÿæˆ {len(slices)} ä¸ªåˆ‡ç‰‡")
    for i, s in enumerate(slices):
        logger.info(f"    åˆ‡ç‰‡ {i+1}: {len(s['layers'])} å±‚, {s['size']/1024:.1f} KB")
    
    return slices


def _split_large_layer(name: str, layer_data: Dict, max_size: int) -> List[Dict]:
    """åˆ‡åˆ†å•ä¸ªè¶…å¤§å±‚"""
    weights = layer_data['weights']
    shape = weights.shape
    
    # ç®€åŒ–ç­–ç•¥ï¼šæ²¿ç¬¬ä¸€ç»´åˆ‡åˆ†
    if len(shape) >= 2:
        dim0_size = shape[0]
        bytes_per_row = weights[0].nbytes
        max_rows = max_size // bytes_per_row
        
        sub_slices = []
        for i in range(0, dim0_size, max_rows):
            end = min(i + max_rows, dim0_size)
            sub_weights = weights[i:end]
            
            sub_slice = {
                'layers': {
                    f"{name}_part{i//max_rows}": {
                        'weights': sub_weights,
                        'scale': layer_data['scale'],
                        'shape': sub_weights.shape,
                        'dtype': 'int8',
                        'is_partial': True,
                        'partial_index': (i, end)
                    }
                },
                'size': sub_weights.nbytes
            }
            sub_slices.append(sub_slice)
        
        logger.info(f"    â†’ åˆ‡åˆ†ä¸º {len(sub_slices)} ä¸ªå­å±‚")
        return sub_slices
    else:
        # æ— æ³•åˆ‡åˆ†ï¼Œè¿”å›åŸå±‚
        return [{'layers': {name: layer_data}, 'size': weights.nbytes}]


# ============================================================================
# Flash å›ºä»¶æ‰“åŒ…
# ============================================================================

def generate_flash_firmware(slices: List[Dict], output_path: str, metadata: Optional[Dict] = None):
    """
    ç”Ÿæˆ Flash å‹å¥½çš„ .bin å›ºä»¶æ–‡ä»¶
    
    å›ºä»¶æ ¼å¼:
    [Header]
        Magic: 4 bytes ('HRF2')
        Version: 2 bytes (0x0210 = 2.1.0)
        Num Slices: 2 bytes
        Total Size: 4 bytes
        Metadata Length: 2 bytes
        Reserved: 2 bytes
    [Metadata JSON]
        å¯å˜é•¿åº¦å…ƒæ•°æ®
    [Slice 0]
        Slice Header: 8 bytes
        Layer Count: 2 bytes
        Reserved: 2 bytes
        Layers...
    [Slice 1]
        ...
    [Padding to Page Boundary]
    """
    logger.info(f"ğŸ“¦ æ‰“åŒ… Flash å›ºä»¶: {output_path}")
    
    with open(output_path, 'wb') as f:
        # ========== å›ºä»¶å¤´éƒ¨ ==========
        f.write(FIRMWARE_MAGIC)  # Magic
        f.write(struct.pack('<H', FIRMWARE_VERSION))  # Version
        f.write(struct.pack('<H', len(slices)))  # Num Slices
        
        # è®¡ç®—æ€»å¤§å°ï¼ˆç¨åå›å¡«ï¼‰
        total_size_offset = f.tell()
        f.write(struct.pack('<I', 0))  # Total Size (placeholder)
        
        # å…ƒæ•°æ®
        if metadata is None:
            metadata = {
                'model_name': 'untitled',
                'timestamp': str(np.datetime64('now')),
                'num_slices': len(slices)
            }
        metadata_json = json.dumps(metadata, indent=None).encode('utf-8')
        f.write(struct.pack('<H', len(metadata_json)))  # Metadata Length
        f.write(struct.pack('<H', 0))  # Reserved
        f.write(metadata_json)  # Metadata
        
        # ========== åˆ‡ç‰‡æ•°æ® ==========
        for slice_idx, slice_data in enumerate(slices):
            slice_start = f.tell()
            
            # åˆ‡ç‰‡å¤´éƒ¨
            f.write(struct.pack('<I', slice_data['size']))  # Slice Size
            f.write(struct.pack('<H', len(slice_data['layers'])))  # Layer Count
            f.write(struct.pack('<H', 0))  # Reserved
            
            # å„å±‚æ•°æ®
            for layer_name, layer_info in slice_data['layers'].items():
                # å±‚å¤´éƒ¨
                layer_name_bytes = layer_name.encode('utf-8')
                f.write(struct.pack('<H', len(layer_name_bytes)))
                f.write(layer_name_bytes)
                
                # å½¢çŠ¶
                shape = layer_info['shape']
                f.write(struct.pack('<B', len(shape)))  # Num Dims
                for dim in shape:
                    f.write(struct.pack('<I', dim))
                
                # Scale
                f.write(struct.pack('<f', layer_info['scale']))
                
                # æƒé‡æ•°æ®
                weights = layer_info['weights']
                f.write(struct.pack('<I', weights.nbytes))
                f.write(weights.tobytes())
        
        # ========== å¯¹é½åˆ° Flash é¡µè¾¹ç•Œ ==========
        current_pos = f.tell()
        padding_size = (FLASH_PAGE_SIZE - (current_pos % FLASH_PAGE_SIZE)) % FLASH_PAGE_SIZE
        if padding_size > 0:
            f.write(b'\xFF' * padding_size)  # Flash æ“¦é™¤é»˜è®¤ 0xFF
        
        # å›å¡«æ€»å¤§å°
        total_size = f.tell()
        f.seek(total_size_offset)
        f.write(struct.pack('<I', total_size))
    
    file_size = os.path.getsize(output_path)
    logger.info(f"  âœ… å›ºä»¶å¤§å°: {file_size} å­—èŠ‚ ({file_size/1024:.1f} KB)")
    logger.info(f"  Flash é¡µæ•°: {file_size // FLASH_PAGE_SIZE} é¡µ (+{file_size % FLASH_PAGE_SIZE} å­—èŠ‚)")
    
    return file_size


# ============================================================================
# å®Œæ•´æµç¨‹
# ============================================================================

def model_to_flash(input_path: str, output_path: str, auto_slice: bool = True, metadata: Optional[Dict] = None):
    """
    æ¨¡å‹ â†’ Flash å›ºä»¶çš„å®Œæ•´æµç¨‹
    
    Args:
        input_path: è¾“å…¥æ¨¡å‹è·¯å¾„ (.pth, .onnx)
        output_path: è¾“å‡º .bin å›ºä»¶è·¯å¾„
        auto_slice: æ˜¯å¦è‡ªåŠ¨åˆ‡ç‰‡å¤§æ¨¡å‹
        metadata: å¯é€‰çš„å…ƒæ•°æ®å­—å…¸
    """
    logger.info("=" * 60)
    logger.info("Hive-Reflex æ¨¡å‹è‡ªåŠ¨åŒ–éƒ¨ç½²å·¥å…·é“¾")
    logger.info("=" * 60)
    
    # Step 1: åŠ è½½æ¨¡å‹
    if input_path.endswith('.onnx'):
        onnx_model = load_onnx_model(input_path)
        weights_dict = extract_onnx_weights(onnx_model)
    elif input_path.endswith(('.pth', '.pt')):
        pytorch_model = load_pytorch_model(input_path)
        weights_dict = {name: param.detach().cpu().numpy() 
                        for name, param in pytorch_model.named_parameters()}
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼: {input_path}")
    
    # Step 2: é‡åŒ–
    quantized_dict = quantize_model(weights_dict)
    
    # Step 3: åˆ‡ç‰‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if auto_slice:
        slices = slice_model_layers(quantized_dict, max_layer_size=MAX_LAYER_SIZE)
    else:
        # å•åˆ‡ç‰‡ï¼ˆæ•´ä¸ªæ¨¡å‹ï¼‰
        total_size = sum(d['weights'].nbytes for d in quantized_dict.values())
        if total_size > CIM_SRAM_SIZE:
            logger.warning(f"âš ï¸  æ¨¡å‹å¤§å° ({total_size/1024:.0f} KB) è¶…è¿‡ SRAM ({CIM_SRAM_SIZE/1024:.0f} KB)ï¼")
            logger.warning("    å»ºè®®ä½¿ç”¨ --auto-slice é€‰é¡¹")
        
        slices = [{'layers': quantized_dict, 'size': total_size}]
    
    # Step 4: ç”Ÿæˆå›ºä»¶
    if metadata is None:
        metadata = {
            'model_name': Path(input_path).stem,
            'input_format': Path(input_path).suffix[1:],  # å»æ‰ '.'
            'cim_sram_size': CIM_SRAM_SIZE,
            'quantization': 'int8',
        }
    
    firmware_size = generate_flash_firmware(slices, output_path, metadata)
    
    logger.info("=" * 60)
    logger.info("âœ… éƒ¨ç½²å®Œæˆ!")
    logger.info(f"    è¾“å…¥: {input_path}")
    logger.info(f"    è¾“å‡º: {output_path} ({firmware_size/1024:.1f} KB)")
    logger.info(f"    åˆ‡ç‰‡: {len(slices)} ä¸ª")
    logger.info("=" * 60)
    
    return output_path


# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='Hive-Reflex æ¨¡å‹è‡ªåŠ¨åŒ–éƒ¨ç½²å·¥å…·é“¾',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # éƒ¨ç½² ONNX æ¨¡å‹
  python model_to_flash.py --input model.onnx --output firmware.bin
  
  # éƒ¨ç½²å¤§æ¨¡å‹ï¼ˆè‡ªåŠ¨åˆ‡ç‰‡ï¼‰
  python model_to_flash.py --input large_model.onnx --output firmware.bin --auto-slice
  
  # æ·»åŠ å…ƒæ•°æ®
  python model_to_flash.py --input model.pth --output firmware.bin --name "MyModel" --version 1.0
        """
    )
    
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥æ¨¡å‹è·¯å¾„ (.pth, .onnx)')
    parser.add_argument('--output', '-o', required=True, help='è¾“å‡ºå›ºä»¶è·¯å¾„ (.bin)')
    parser.add_argument('--auto-slice', action='store_true', help='è‡ªåŠ¨åˆ‡ç‰‡å¤§æ¨¡å‹')
    parser.add_argument('--max-slice', type=int, default=MAX_LAYER_SIZE, help='æœ€å¤§åˆ‡ç‰‡å¤§å°ï¼ˆå­—èŠ‚ï¼‰')
    parser.add_argument('--name', help='æ¨¡å‹åç§°ï¼ˆå…ƒæ•°æ®ï¼‰')
    parser.add_argument('--version', help='æ¨¡å‹ç‰ˆæœ¬ï¼ˆå…ƒæ•°æ®ï¼‰')
    
    args = parser.parse_args()
    
    # æ„å»ºå…ƒæ•°æ®
    metadata = {}
    if args.name:
        metadata['model_name'] = args.name
    if args.version:
        metadata['model_version'] = args.version
    
    # æ‰§è¡Œéƒ¨ç½²
    try:
        model_to_flash(
            args.input,
            args.output,
            auto_slice=args.auto_slice,
            metadata=metadata if metadata else None
        )
    except Exception as e:
        logger.error(f"âŒ éƒ¨ç½²å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == '__main__':
    exit(main())
