"""
ReflexNet å‡çº§ç‰ˆ - æ”¯æŒ MLIR ç¼–è¯‘å™¨å¯¼å‡º
æ•´åˆé‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT) å’Œ CIM ä¼˜åŒ–
"""

import torch
import torch.nn as nn
import argparse

class ReflexNetV2(nn.Module):
    """
    ReflexNet 2.0 - ä¼˜åŒ–ç”¨äº Digital CIM æ¶æ„
    
    æ”¹è¿›:
    - é‡åŒ–å‹å¥½çš„å±‚è®¾è®¡
    - é€‚é… CIM çŸ©é˜µç»´åº¦
    - æ”¯æŒ MLIR å¯¼å‡º
    """
    def __init__(self, quantize=False):
        super(ReflexNetV2, self).__init__()
        
        # è¾“å…¥ç»´åº¦: 12 (6 IMU + 3 Hist + 2 Current + 1 TargetDiff)
        # ä¼˜åŒ–ä¸º CIM å‹å¥½çš„ç»´åº¦ (32çš„å€æ•°)
        
        self.fc1 = nn.Linear(12, 32)
        self.relu = nn.ReLU()
        
        # LSTM å•å…ƒ
        self.lstm = nn.LSTM(input_size=32, hidden_size=16, batch_first=True)
        
        self.fc2 = nn.Linear(16, 1)
        self.tanh = nn.Tanh()
        
        # é‡åŒ–é…ç½®
        if quantize:
            self.quant = torch.quantization.QuantStub()
            self.dequant = torch.quantization.DeQuantStub()
        else:
            self.quant = None
            self.dequant = None

    def forward(self, x, hidden_state=None):
        # x shape: (batch, seq_len, 12)
        
        if self.quant:
            x = self.quant(x)
        
        x = self.fc1(x)
        x = self.relu(x)
        
        # LSTM å¤„ç†
        if hidden_state is None:
            x, new_hidden = self.lstm(x)
        else:
            x, new_hidden = self.lstm(x, hidden_state)
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        last_step = x[:, -1, :]
        
        out = self.fc2(last_step)
        out = self.tanh(out)
        
        if self.dequant:
            out = self.dequant(out)
        
        return out, new_hidden


def export_to_onnx(model_path='reflex_net_v2.onnx', quantize=False):
    """å¯¼å‡ºä¸º ONNX æ ¼å¼"""
    print(f"ğŸ”¨ å¯¼å‡º ReflexNet V2 â†’ ONNX")
    
    model = ReflexNetV2(quantize=quantize)
    model.eval()
    
    # ç»Ÿè®¡å‚æ•°
    param_count = sum(p.numel() for p in model.parameters())
    param_size_kb = param_count * 4 / 1024  # FP32
    
    print(f"  å‚æ•°é‡: {param_count} ({param_size_kb:.2f} KB)")
    
    # å‡†å¤‡ç¤ºä¾‹è¾“å…¥
    dummy_input = torch.randn(1, 5, 12)  # Batch=1, Seq=5, Feat=12
    h0 = torch.zeros(1, 1, 16)
    c0 = torch.zeros(1, 1, 16)
    
    # é‡åŒ– (å¦‚æœéœ€è¦)
    if quantize:
        print("  åº”ç”¨åŠ¨æ€é‡åŒ– (INT8)...")
        model = torch.quantization.quantize_dynamic(
            model, {nn.Linear, nn.LSTM}, dtype=torch.qint8
        )
        print(f"  é‡åŒ–åæ¨¡å‹å¤§å°çº¦ä¸º {param_size_kb / 4:.2f} KB")
    
    # å¯¼å‡º
    print(f"  å¯¼å‡ºæ–‡ä»¶: {model_path}")
    torch.onnx.export(
        model,
        (dummy_input, (h0, c0)),
        model_path,
        input_names=['input', 'h_in', 'c_in'],
        output_names=['output', 'h_out', 'c_out'],
        opset_version=11,
        dynamic_axes={'input': {0: 'batch', 1: 'seq_len'}}
    )
    
    print("  âœ“ ONNX å¯¼å‡ºå®Œæˆ")
    return model_path


def export_to_mlir(model, output_path='reflex_net_v2.mlir'):
    """
    å¯¼å‡ºä¸º MLIR æ ¼å¼ (éœ€è¦ torch-mlir)
    è¿™æ˜¯æœªæ¥çš„ç›®æ ‡ï¼Œå½“å‰ä½¿ç”¨ ONNX ä½œä¸ºä¸­é—´æ ¼å¼
    """
    try:
        import torch_mlir
        
        print(f"ğŸ”¨ å¯¼å‡º ReflexNet V2 â†’ MLIR")
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        example_input = torch.randn(1, 5, 12)
        
        # ç¼–è¯‘ä¸º MLIR
        mlir_module = torch_mlir.compile(
            model,
            example_input,
            output_type="torch"
        )
        
        # ä¿å­˜ MLIR IR
        with open(output_path, 'w') as f:
            f.write(str(mlir_module))
        
        print(f"  âœ“ MLIR å¯¼å‡ºå®Œæˆ: {output_path}")
        
    except ImportError:
        print("  âš ï¸  torch-mlir æœªå®‰è£…ï¼Œä½¿ç”¨ ONNX ä½œä¸ºæ›¿ä»£")
        return export_to_onnx()


def compile_for_cim(onnx_path):
    """
    ä½¿ç”¨ MLIR ç¼–è¯‘å™¨ç¼–è¯‘ä¸º CIM ç›®æ ‡ä»£ç 
    è¿™ä¼šè°ƒç”¨ mlir_compiler/compile.py
    """
    import subprocess
    import os
    
    print("\nğŸš€ ä½¿ç”¨ MLIR ç¼–è¯‘å™¨ç¼–è¯‘...")
    
    compiler_script = os.path.join(
        os.path.dirname(__file__),
        'mlir_compiler',
        'compile.py'
    )
    
    cmd = [
        'python3',
        compiler_script,
        '--model', onnx_path,
        '--output-c', 'build/reflex_inference.c',
        '--output-weights', 'build/reflex_weights.bin'
    ]
    
    subprocess.run(cmd, check=True)
    print("  âœ“ CIM ç¼–è¯‘å®Œæˆ")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='ReflexNet V2 å¯¼å‡ºå·¥å…·')
    parser.add_argument('--quantize', action='store_true', help='å¯¼å‡ºé‡åŒ–æ¨¡å‹')
    parser.add_argument('--mlir', action='store_true', help='å¯¼å‡º MLIR æ ¼å¼')
    parser.add_argument('--compile-cim', action='store_true', help='ç¼–è¯‘ä¸º CIM ä»£ç ')
    
    args = parser.parse_args()
    
    # å¯¼å‡ºæ¨¡å‹
    if args.mlir:
        model = ReflexNetV2(quantize=args.quantize)
        export_to_mlir(model)
    else:
        onnx_path = export_to_onnx(quantize=args.quantize)
        
        # å¦‚æœéœ€è¦ï¼Œç¼–è¯‘ä¸º CIM ä»£ç 
        if args.compile_cim:
            compile_for_cim(onnx_path)
    
    print("\nâœ… å®Œæˆ!")
    if args.compile_cim:
        print("\nä¸‹ä¸€æ­¥:")
        print("  make APP_SRCS='examples/example_reflex_node.c build/reflex_inference.c'")
