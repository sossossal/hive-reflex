#!/usr/bin/env python3
"""
é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰æ¨¡å—
æ”¯æŒä¼ªé‡åŒ–è®­ç»ƒã€Conv+BN èåˆå’Œè‡ªåŠ¨ç²¾åº¦æŸå¤±è¡¥å¿

@file qat_trainer.py
@version 2.1.0
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from typing import Dict, List, Tuple, Optional
import numpy as np
import logging
import copy

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================================================
# ä¼ªé‡åŒ–å±‚
# ============================================================================

class FakeQuantize(nn.Module):
    """
    ä¼ªé‡åŒ–å±‚ - å¯å¾®åˆ†çš„é‡åŒ–æ¨¡æ‹Ÿ
    
    å‰å‘ä¼ æ’­ä¸­æ¨¡æ‹Ÿé‡åŒ–è¯¯å·®ï¼Œåå‘ä¼ æ’­ä½¿ç”¨ STEï¼ˆç›´é€šä¼°è®¡å™¨ï¼‰
    """
    
    def __init__(self, num_bits: int = 8, symmetric: bool = True,
                 per_channel: bool = False, learnable: bool = True):
        super().__init__()
        self.num_bits = num_bits
        self.symmetric = symmetric
        self.per_channel = per_channel
        self.learnable = learnable
        
        # é‡åŒ–èŒƒå›´
        if symmetric:
            self.q_min = -(2 ** (num_bits - 1))
            self.q_max = 2 ** (num_bits - 1) - 1
        else:
            self.q_min = 0
            self.q_max = 2 ** num_bits - 1
        
        # å¯å­¦ä¹ çš„ scale å’Œ zero_point
        if learnable:
            self.register_parameter('scale', nn.Parameter(torch.ones(1)))
            self.register_parameter('zero_point', nn.Parameter(torch.zeros(1)))
        else:
            self.register_buffer('scale', torch.ones(1))
            self.register_buffer('zero_point', torch.zeros(1))
        
        # æ ¡å‡†ç»Ÿè®¡
        self.register_buffer('min_val', torch.zeros(1))
        self.register_buffer('max_val', torch.zeros(1))
        self.register_buffer('calibrated', torch.tensor(False))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.training and not self.calibrated:
            # æ ¡å‡†æ¨¡å¼ï¼šæ”¶é›†ç»Ÿè®¡ä¿¡æ¯
            self._update_calibration(x)
        
        # ä¼ªé‡åŒ–
        return self._fake_quantize(x)
    
    def _update_calibration(self, x: torch.Tensor):
        """æ›´æ–°æ ¡å‡†ç»Ÿè®¡"""
        with torch.no_grad():
            if self.per_channel and x.dim() >= 2:
                min_val = x.min(dim=0)[0]
                max_val = x.max(dim=0)[0]
            else:
                min_val = x.min()
                max_val = x.max()
            
            # EMA æ›´æ–°
            if self.min_val.numel() == 1:
                self.min_val = min_val.clone()
                self.max_val = max_val.clone()
            else:
                self.min_val = 0.9 * self.min_val + 0.1 * min_val
                self.max_val = 0.9 * self.max_val + 0.1 * max_val
    
    def _fake_quantize(self, x: torch.Tensor) -> torch.Tensor:
        """æ‰§è¡Œä¼ªé‡åŒ–ï¼ˆå¯å¾®åˆ†ï¼‰"""
        # è®¡ç®— scale
        if self.symmetric:
            abs_max = torch.max(torch.abs(self.min_val), torch.abs(self.max_val))
            scale = abs_max / self.q_max
        else:
            scale = (self.max_val - self.min_val) / (self.q_max - self.q_min)
        
        scale = torch.clamp(scale, min=1e-8)
        
        # é‡åŒ–å’Œåé‡åŒ–
        x_q = torch.clamp(
            torch.round(x / scale) + self.zero_point,
            self.q_min, self.q_max
        )
        x_dq = (x_q - self.zero_point) * scale
        
        # STE: å‰å‘ç”¨é‡åŒ–å€¼ï¼Œåå‘ç”¨åŸå§‹æ¢¯åº¦
        return x + (x_dq - x).detach()
    
    def finish_calibration(self):
        """å®Œæˆæ ¡å‡†"""
        self.calibrated = torch.tensor(True)
        
        if self.symmetric:
            abs_max = torch.max(torch.abs(self.min_val), torch.abs(self.max_val))
            self.scale.data = abs_max / self.q_max
        else:
            self.scale.data = (self.max_val - self.min_val) / (self.q_max - self.q_min)


# ============================================================================
# é‡åŒ–æ„ŸçŸ¥å±‚åŒ…è£…å™¨
# ============================================================================

class QuantizedConv2d(nn.Module):
    """é‡åŒ–æ„ŸçŸ¥ Conv2d"""
    
    def __init__(self, conv: nn.Conv2d, num_bits: int = 8):
        super().__init__()
        self.conv = conv
        self.weight_quantizer = FakeQuantize(num_bits, symmetric=True, learnable=True)
        self.activation_quantizer = FakeQuantize(num_bits, symmetric=False, learnable=True)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # é‡åŒ–æƒé‡
        q_weight = self.weight_quantizer(self.conv.weight)
        
        # ä½¿ç”¨é‡åŒ–æƒé‡è¿›è¡Œå·ç§¯
        out = F.conv2d(x, q_weight, self.conv.bias, self.conv.stride,
                       self.conv.padding, self.conv.dilation, self.conv.groups)
        
        # é‡åŒ–æ¿€æ´»
        return self.activation_quantizer(out)


class QuantizedLinear(nn.Module):
    """é‡åŒ–æ„ŸçŸ¥ Linear"""
    
    def __init__(self, linear: nn.Linear, num_bits: int = 8):
        super().__init__()
        self.linear = linear
        self.weight_quantizer = FakeQuantize(num_bits, symmetric=True, learnable=True)
        self.activation_quantizer = FakeQuantize(num_bits, symmetric=False, learnable=True)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        q_weight = self.weight_quantizer(self.linear.weight)
        out = F.linear(x, q_weight, self.linear.bias)
        return self.activation_quantizer(out)


# ============================================================================
# Conv+BN èåˆ
# ============================================================================

def fuse_conv_bn(conv: nn.Conv2d, bn: nn.BatchNorm2d) -> nn.Conv2d:
    """
    èåˆ Conv2d å’Œ BatchNorm2d
    
    èåˆåçš„æƒé‡: w_fused = w * gamma / sqrt(var + eps)
    èåˆåçš„åç½®: b_fused = (b - mean) * gamma / sqrt(var + eps) + beta
    """
    # è·å– BN å‚æ•°
    gamma = bn.weight
    beta = bn.bias
    mean = bn.running_mean
    var = bn.running_var
    eps = bn.eps
    
    # è®¡ç®—èåˆå› å­
    std = torch.sqrt(var + eps)
    scale = gamma / std
    
    # èåˆæƒé‡
    fused_weight = conv.weight * scale.view(-1, 1, 1, 1)
    
    # èåˆåç½®
    if conv.bias is not None:
        fused_bias = (conv.bias - mean) * scale + beta
    else:
        fused_bias = -mean * scale + beta
    
    # åˆ›å»ºèåˆåçš„ Conv
    fused_conv = nn.Conv2d(
        conv.in_channels, conv.out_channels, conv.kernel_size,
        conv.stride, conv.padding, conv.dilation, conv.groups,
        bias=True
    )
    
    fused_conv.weight.data = fused_weight
    fused_conv.bias.data = fused_bias
    
    return fused_conv


def fuse_model_conv_bn(model: nn.Module) -> nn.Module:
    """
    èåˆæ¨¡å‹ä¸­æ‰€æœ‰çš„ Conv+BN å¯¹
    """
    logger.info("ğŸ”§ èåˆ Conv+BatchNorm å±‚...")
    
    fused_count = 0
    prev_name = None
    prev_module = None
    modules_to_fuse = []
    
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d):
            prev_name = name
            prev_module = module
        elif isinstance(module, nn.BatchNorm2d) and prev_module is not None:
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…
            if module.num_features == prev_module.out_channels:
                modules_to_fuse.append((prev_name, name, prev_module, module))
                fused_count += 1
        else:
            prev_name = None
            prev_module = None
    
    # æ‰§è¡Œèåˆ
    for conv_name, bn_name, conv, bn in modules_to_fuse:
        fused = fuse_conv_bn(conv, bn)
        
        # æ›¿æ¢æ¨¡å—
        parent_name = conv_name.rsplit('.', 1)
        if len(parent_name) == 2:
            parent = dict(model.named_modules())[parent_name[0]]
            setattr(parent, parent_name[1], fused)
        else:
            setattr(model, conv_name, fused)
        
        # ç§»é™¤ BNï¼ˆæ›¿æ¢ä¸º Identityï¼‰
        bn_parent_name = bn_name.rsplit('.', 1)
        if len(bn_parent_name) == 2:
            parent = dict(model.named_modules())[bn_parent_name[0]]
            setattr(parent, bn_parent_name[1], nn.Identity())
        else:
            setattr(model, bn_name, nn.Identity())
        
        logger.info(f"  èåˆ: {conv_name} + {bn_name}")
    
    logger.info(f"âœ“ èåˆå®Œæˆï¼Œå…± {fused_count} å¯¹")
    return model


# ============================================================================
# QAT è®­ç»ƒå™¨
# ============================================================================

class QATTrainer:
    """
    é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå™¨
    
    æ”¯æŒï¼š
    - ä¼ªé‡åŒ–è®­ç»ƒ
    - å¯å­¦ä¹ çš„é‡åŒ–å‚æ•°
    - è‡ªåŠ¨å±‚æ›¿æ¢
    - æ•æ„Ÿå±‚ä¿æŠ¤
    """
    
    def __init__(self, model: nn.Module, dataloader, 
                 num_bits: int = 8, lr: float = 1e-4,
                 sensitive_layers: Optional[List[str]] = None):
        """
        åˆå§‹åŒ– QAT è®­ç»ƒå™¨
        
        Args:
            model: åŸå§‹ FP32 æ¨¡å‹
            dataloader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            num_bits: é‡åŒ–ä½æ•°
            lr: å­¦ä¹ ç‡
            sensitive_layers: æ•æ„Ÿå±‚åç§°åˆ—è¡¨ï¼ˆä¿æŒ FP16ï¼‰
        """
        self.original_model = model
        self.dataloader = dataloader
        self.num_bits = num_bits
        self.lr = lr
        self.sensitive_layers = sensitive_layers or []
        
        # åˆ›å»º QAT æ¨¡å‹
        self.qat_model = self._prepare_qat_model(copy.deepcopy(model))
        self.optimizer = Adam(self.qat_model.parameters(), lr=lr)
        
        # è®­ç»ƒç»Ÿè®¡
        self.epoch_losses = []
        self.calibration_done = False
    
    def _prepare_qat_model(self, model: nn.Module) -> nn.Module:
        """å‡†å¤‡ QAT æ¨¡å‹ï¼ˆæ›¿æ¢å±‚ä¸ºé‡åŒ–ç‰ˆæœ¬ï¼‰"""
        logger.info("ğŸ”¨ å‡†å¤‡ QAT æ¨¡å‹...")
        
        # é¦–å…ˆèåˆ Conv+BN
        model = fuse_model_conv_bn(model)
        
        # æ›¿æ¢å±‚
        replaced = 0
        for name, module in list(model.named_modules()):
            # è·³è¿‡æ•æ„Ÿå±‚
            if any(s in name for s in self.sensitive_layers):
                logger.info(f"  è·³è¿‡æ•æ„Ÿå±‚: {name}")
                continue
            
            parent_name = name.rsplit('.', 1)
            if len(parent_name) == 2:
                parent = dict(model.named_modules())[parent_name[0]]
                child_name = parent_name[1]
            else:
                parent = model
                child_name = name
            
            # æ›¿æ¢ Conv2d
            if isinstance(module, nn.Conv2d) and not isinstance(module, QuantizedConv2d):
                setattr(parent, child_name, QuantizedConv2d(module, self.num_bits))
                replaced += 1
            
            # æ›¿æ¢ Linear
            elif isinstance(module, nn.Linear) and not isinstance(module, QuantizedLinear):
                setattr(parent, child_name, QuantizedLinear(module, self.num_bits))
                replaced += 1
        
        logger.info(f"âœ“ æ›¿æ¢ {replaced} å±‚ä¸ºé‡åŒ–ç‰ˆæœ¬")
        return model
    
    def calibrate(self, num_batches: int = 100):
        """æ ¡å‡†æ¨¡å‹ï¼ˆæ”¶é›†æ¿€æ´»èŒƒå›´ï¼‰"""
        logger.info("ğŸ“ æ ¡å‡†é‡åŒ–å‚æ•°...")
        
        self.qat_model.train()  # éœ€è¦ train æ¨¡å¼æ›´æ–°ç»Ÿè®¡
        
        with torch.no_grad():
            for i, (inputs, _) in enumerate(self.dataloader):
                if i >= num_batches:
                    break
                self.qat_model(inputs)
        
        # å®Œæˆæ ¡å‡†
        for module in self.qat_model.modules():
            if isinstance(module, FakeQuantize):
                module.finish_calibration()
        
        self.calibration_done = True
        logger.info("âœ“ æ ¡å‡†å®Œæˆ")
    
    def train_epoch(self, criterion=None) -> float:
        """è®­ç»ƒä¸€ä¸ª epoch"""
        if criterion is None:
            criterion = nn.CrossEntropyLoss()
        
        self.qat_model.train()
        total_loss = 0
        num_batches = 0
        
        for inputs, targets in self.dataloader:
            self.optimizer.zero_grad()
            
            outputs = self.qat_model(inputs)
            loss = criterion(outputs, targets)
            
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
        
        avg_loss = total_loss / max(num_batches, 1)
        self.epoch_losses.append(avg_loss)
        
        return avg_loss
    
    def train(self, epochs: int = 10, criterion=None) -> List[float]:
        """å®Œæ•´ QAT è®­ç»ƒ"""
        logger.info(f"ğŸš€ å¼€å§‹ QAT è®­ç»ƒ ({epochs} epochs)")
        
        if not self.calibration_done:
            self.calibrate()
        
        for epoch in range(epochs):
            loss = self.train_epoch(criterion)
            logger.info(f"  Epoch {epoch+1}/{epochs}: Loss = {loss:.4f}")
        
        logger.info("âœ“ QAT è®­ç»ƒå®Œæˆ")
        return self.epoch_losses
    
    def export_quantized_model(self, output_path: str,
                               export_format: str = 'onnx') -> None:
        """å¯¼å‡ºé‡åŒ–æ¨¡å‹"""
        logger.info(f"ğŸ“¦ å¯¼å‡ºé‡åŒ–æ¨¡å‹: {output_path}")
        
        self.qat_model.eval()
        
        if export_format == 'onnx':
            # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
            dummy_input = torch.randn(1, 3, 224, 224)
            
            # è½¬æ¢ä¸ºé™æ€é‡åŒ–
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„è½¬æ¢
            torch.onnx.export(
                self.qat_model,
                dummy_input,
                output_path,
                opset_version=13,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
        elif export_format == 'torch':
            torch.save(self.qat_model.state_dict(), output_path)
        
        logger.info("âœ“ å¯¼å‡ºå®Œæˆ")
    
    def get_quantization_config(self) -> Dict:
        """è·å–é‡åŒ–é…ç½®ï¼ˆç”¨äº CIM ä»£ç ç”Ÿæˆï¼‰"""
        config = {
            'num_bits': self.num_bits,
            'layers': []
        }
        
        for name, module in self.qat_model.named_modules():
            if isinstance(module, (QuantizedConv2d, QuantizedLinear)):
                layer_config = {
                    'name': name,
                    'weight_scale': module.weight_quantizer.scale.item(),
                    'activation_scale': module.activation_quantizer.scale.item()
                }
                config['layers'].append(layer_config)
        
        return config


# ============================================================================
# è‡ªåŠ¨ç²¾åº¦æŸå¤±è¡¥å¿
# ============================================================================

def compute_quantization_error(fp_model: nn.Module, qat_model: nn.Module,
                               test_loader, num_batches: int = 50) -> Dict:
    """
    è®¡ç®—é‡åŒ–è¯¯å·®
    """
    fp_model.eval()
    qat_model.eval()
    
    errors = []
    
    with torch.no_grad():
        for i, (inputs, _) in enumerate(test_loader):
            if i >= num_batches:
                break
            
            fp_out = fp_model(inputs)
            qat_out = qat_model(inputs)
            
            mae = torch.abs(fp_out - qat_out).mean().item()
            mse = ((fp_out - qat_out) ** 2).mean().item()
            
            errors.append({'mae': mae, 'mse': mse})
    
    avg_mae = np.mean([e['mae'] for e in errors])
    avg_mse = np.mean([e['mse'] for e in errors])
    
    return {
        'mae': avg_mae,
        'mse': avg_mse,
        'rmse': np.sqrt(avg_mse),
        'snr_db': 10 * np.log10(1 / max(avg_mse, 1e-10))
    }


def auto_precision_compensation(trainer: QATTrainer, test_loader,
                                 target_error: float = 0.01,
                                 max_iterations: int = 5) -> None:
    """
    è‡ªåŠ¨ç²¾åº¦æŸå¤±è¡¥å¿
    
    å¦‚æœé‡åŒ–è¯¯å·®è¶…è¿‡ç›®æ ‡ï¼Œè‡ªåŠ¨è°ƒæ•´æ•æ„Ÿå±‚ç²¾åº¦
    """
    logger.info("ğŸ¯ è‡ªåŠ¨ç²¾åº¦è¡¥å¿...")
    
    for iteration in range(max_iterations):
        error = compute_quantization_error(
            trainer.original_model, trainer.qat_model, test_loader
        )
        
        logger.info(f"  è¿­ä»£ {iteration+1}: MAE={error['mae']:.4f}, SNR={error['snr_db']:.1f}dB")
        
        if error['mae'] < target_error:
            logger.info("âœ“ è¾¾åˆ°ç›®æ ‡ç²¾åº¦")
            break
        
        # å¢åŠ è®­ç»ƒè½®æ¬¡
        trainer.train(epochs=5)
    
    logger.info("âœ“ ç²¾åº¦è¡¥å¿å®Œæˆ")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='QAT è®­ç»ƒå™¨')
    parser.add_argument('--model', required=True, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--output', required=True, help='è¾“å‡ºè·¯å¾„')
    parser.add_argument('--epochs', type=int, default=10, help='è®­ç»ƒè½®æ¬¡')
    parser.add_argument('--bits', type=int, default=8, help='é‡åŒ–ä½æ•°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--fuse-bn', action='store_true', help='èåˆ BatchNorm')
    
    args = parser.parse_args()
    
    print("=" * 50)
    print("Hive-Reflex QAT è®­ç»ƒå™¨")
    print("=" * 50)
    print(f"æ¨¡å‹: {args.model}")
    print(f"é‡åŒ–ä½æ•°: {args.bits}")
    print(f"è®­ç»ƒè½®æ¬¡: {args.epochs}")
    
    # TODO: åŠ è½½æ¨¡å‹å’Œæ•°æ®ï¼Œæ‰§è¡Œè®­ç»ƒ
    
    print("\nâœ… QAT è®­ç»ƒå®Œæˆ!")


if __name__ == '__main__':
    main()
