# Hive-Reflex æ¨¡å‹éƒ¨ç½²å·¥å…·é“¾ä½¿ç”¨æŒ‡å—

## æ¦‚è§ˆ

`model_to_flash.py` æ˜¯ä¸€ä¸ª**ç«¯åˆ°ç«¯çš„è‡ªåŠ¨åŒ–éƒ¨ç½²å·¥å…·**ï¼Œå¯ä»¥å°† PyTorch æˆ– ONNX æ¨¡å‹è‡ªåŠ¨è½¬æ¢ä¸º Hive-Reflex èŠ¯ç‰‡å¯ç”¨çš„ Flash å›ºä»¶ã€‚

### æ ¸å¿ƒåŠŸèƒ½

1. âœ‚ï¸ **è‡ªåŠ¨åˆ‡ç‰‡** - å°†è¶…è¿‡ 512 KB çš„å¤§æ¨¡å‹åˆ‡åˆ†ä¸ºå¤šä¸ªå¯åŠ è½½çš„åˆ‡ç‰‡
2. ğŸ”¢ **INT8 é‡åŒ–** - è‡ªåŠ¨é‡åŒ–æƒé‡ä»¥å‡å°æ¨¡å‹å°ºå¯¸ï¼ˆ4x å‹ç¼©ï¼‰
3. ğŸ“¦ **Flash æ‰“åŒ…** - ç”Ÿæˆå¯¹é½ Flash é¡µè¾¹ç•Œçš„ .bin å›ºä»¶æ–‡ä»¶

---

## å¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡æ¨¡å‹

æ”¯æŒçš„æ ¼å¼ï¼š
- **PyTorch**: `.pth` æˆ– `.pt` (å®Œæ•´æ¨¡å‹ï¼Œé state_dict)
- **ONNX**: `.onnx`

```python
# PyTorch ç¤ºä¾‹ï¼šä¿å­˜å®Œæ•´æ¨¡å‹
import torch

model = MyModel()
torch.save(model, 'my_model.pth')  # âœ… æ­£ç¡®

# âŒ é”™è¯¯ï¼šä¸è¦åªä¿å­˜ state_dict
# torch.save(model.state_dict(), 'my_model.pth')
```

### 2. è¿è¡Œéƒ¨ç½²å·¥å…·

```bash
# åŸºæœ¬ç”¨æ³•
python tools/model_to_flash.py --input my_model.onnx --output firmware.bin

# è‡ªåŠ¨åˆ‡ç‰‡å¤§æ¨¡å‹
python tools/model_to_flash.py --input large_model.onnx --output firmware.bin --auto-slice

# æ·»åŠ å…ƒæ•°æ®
python tools/model_to_flash.py \
    --input model.pth \
    --output firmware.bin \
    --name "GestureRecognition" \
    --version 1.0
```

### 3. å·¥å…·è¾“å‡ºç¤ºä¾‹

```
=============================================================
Hive-Reflex æ¨¡å‹è‡ªåŠ¨åŒ–éƒ¨ç½²å·¥å…·é“¾
=============================================================
INFO: ğŸ“¦ åŠ è½½ ONNX æ¨¡å‹: model.onnx
INFO:   æå– 12 ä¸ªæƒé‡å¼ é‡
INFO: ğŸ”¢ é‡åŒ–æ¨¡å‹æƒé‡åˆ° INT8...
INFO:   åŸå§‹å¤§å°: 850.0 KB
INFO:   é‡åŒ–å: 212.5 KB
INFO:   å‹ç¼©æ¯”: 4.00x
INFO: âœ‚ï¸  åˆ‡ç‰‡æ¨¡å‹ (æœ€å¤§åˆ‡ç‰‡: 256 KB)...
INFO:   ç”Ÿæˆ 1 ä¸ªåˆ‡ç‰‡
INFO:     åˆ‡ç‰‡ 1: 12 å±‚, 212.5 KB
INFO: ğŸ“¦ æ‰“åŒ… Flash å›ºä»¶: firmware.bin
INFO:   âœ… å›ºä»¶å¤§å°: 217088 å­—èŠ‚ (212.0 KB)
INFO:   Flash é¡µæ•°: 53 é¡µ (+0 å­—èŠ‚)
=============================================================
âœ… éƒ¨ç½²å®Œæˆ!
    è¾“å…¥: model.onnx
    è¾“å‡º: firmware.bin (212.0 KB)
    åˆ‡ç‰‡: 1 ä¸ª
=============================================================
```

---

## åµŒå…¥å¼ç«¯åŠ è½½

ç”Ÿæˆçš„ `.bin` å›ºä»¶å¯ç›´æ¥çƒ§å½•åˆ° Flashï¼Œç„¶åä½¿ç”¨ `flash_loader.c` åŠ è½½åˆ° CIM SRAMã€‚

### ç¤ºä¾‹ä»£ç 

```c
#include "flash_loader.h"
#include "imc22_cim.h"

#define FLASH_MODEL_ADDR 0x10000000  // Flash ä¸­æ¨¡å‹èµ·å§‹åœ°å€
#define CIM_SRAM_BASE    0x80000000  // CIM SRAM åŸºåœ°å€

int main(void) {
    // 1. éªŒè¯å›ºä»¶
    if (flash_firmware_validate(FLASH_MODEL_ADDR) != 0) {
        printf("å›ºä»¶éªŒè¯å¤±è´¥!\n");
        return -1;
    }
    
    // 2. åŠ è½½æ¨¡å‹åˆ° SRAM
    if (flash_load_full_model(FLASH_MODEL_ADDR, CIM_SRAM_BASE) != 0) {
        printf("æ¨¡å‹åŠ è½½å¤±è´¥!\n");
        return -1;
    }
    
    // 3. åˆå§‹åŒ– CIM å¼•æ“
    imc22_init();
    
    // 4. è¿è¡Œæ¨ç†
    float input[8] = {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8};
    float output[2];
    
    imc22_inference(input, output, 8, 2);
    
    printf("è¾“å‡º: [%.3f, %.3f]\n", output[0], output[1]);
    
    return 0;
}
```

---

## é«˜çº§åŠŸèƒ½

### 1. å¤šåˆ‡ç‰‡æ¨¡å‹ï¼ˆç”¨äºè¶…å¤§æ¨¡å‹ï¼‰

å¦‚æœæ¨¡å‹è¶…è¿‡ 512 KBï¼Œå·¥å…·ä¼šè‡ªåŠ¨åˆ‡ç‰‡ã€‚åœ¨åµŒå…¥å¼ç«¯éœ€è¦æŒ‰éœ€åŠ è½½åˆ‡ç‰‡ï¼š

```c
uint16_t num_slices = flash_get_num_slices();

for (uint8_t i = 0; i < num_slices; i++) {
    // åŠ è½½åˆ‡ç‰‡ i
    flash_load_slice(i, CIM_SRAM_BASE);
    
    // è¿è¡Œè¯¥åˆ‡ç‰‡çš„æ¨ç†
    run_inference_slice(i);
}
```

### 2. æŸ¥çœ‹å›ºä»¶ä¿¡æ¯

ä½¿ç”¨ Python è„šæœ¬æŸ¥çœ‹ `.bin` å›ºä»¶çš„è¯¦ç»†ä¿¡æ¯ï¼š

```python
import struct

with open('firmware.bin', 'rb') as f:
    magic = f.read(4)
    version = struct.unpack('<H', f.read(2))[0]
    num_slices = struct.unpack('<H', f.read(2))[0]
    total_size = struct.unpack('<I', f.read(4))[0]
    
    print(f"Magic: {magic}")
    print(f"Version: {version >> 8}.{(version >> 4) & 0xF}.0")
    print(f"Slices: {num_slices}")
    print(f"Size: {total_size} bytes")
```

### 3. è‡ªå®šä¹‰é‡åŒ–å‚æ•°

ä¿®æ”¹ `model_to_flash.py` ä¸­çš„å¸¸é‡ï¼š

```python
# è°ƒæ•´åˆ‡ç‰‡å¤§å°ï¼ˆé»˜è®¤ 256 KBï¼‰
MAX_LAYER_SIZE = 128 * 1024  # 128 KB åˆ‡ç‰‡

# è°ƒæ•´ Flash é¡µå¤§å°ï¼ˆæ ¹æ®å®é™… Flash èŠ¯ç‰‡ï¼‰
FLASH_PAGE_SIZE = 4096  # æˆ– 2048, 8192 ç­‰
```

---

## å›ºä»¶æ ¼å¼è§„èŒƒ

### å›ºä»¶å¸ƒå±€

```
+----------------------------+
| Header (16 bytes)          |
|  - Magic: 'HRF2'           |
|  - Version: 0x0210         |
|  - Num Slices              |
|  - Total Size              |
+----------------------------+
| Metadata (JSON, å¯å˜é•¿)    |
+----------------------------+
| Slice 0                    |
|  - Slice Header            |
|  - Layer 0                 |
|    - Name                  |
|    - Shape                 |
|    - Scale                 |
|    - Weights (INT8[])      |
|  - Layer 1                 |
|  ...                       |
+----------------------------+
| Slice 1                    |
|  ...                       |
+----------------------------+
| Padding (å¯¹é½ Flash Page)  |
+----------------------------+
```

### æ•°æ®ç±»å‹

- **æƒé‡**: INT8 (å¯¹ç§°é‡åŒ–ï¼ŒèŒƒå›´ [-127, 127])
- **Scale**: FLOAT32
- **å½¢çŠ¶**: UINT32[]

---

## å®Œæ•´å·¥ä½œæµç¤ºä¾‹

### åœºæ™¯ï¼šéƒ¨ç½²æ‰‹åŠ¿è¯†åˆ«æ¨¡å‹

```bash
# 1. è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨ç°æœ‰å·¥å…·ï¼‰
python tools/train_adaptive_model.py --all

# 2. å¯¼å‡ºä¸º ONNX
python -c "
import torch
import torch.onnx

model = torch.load('models/adaptive_model.pt')
dummy_input = torch.randn(1, 8)
torch.onnx.export(model, dummy_input, 'models/gesture_model.onnx')
"

# 3. ç”Ÿæˆ Flash å›ºä»¶
python tools/model_to_flash.py \
    --input models/gesture_model.onnx \
    --output fpga/firmware/gesture_model.bin \
    --name "GestureNet" \
    --version 2.1

# 4. çƒ§å½•åˆ° Flashï¼ˆä½¿ç”¨ Vivado æˆ– JTAGï¼‰
vivado -mode batch -source fpga/program_flash.tcl

# 5. è¿è¡ŒåµŒå…¥å¼å›ºä»¶
cd fpga
make run_test
```

---

## å¸¸è§é—®é¢˜

### Q1: æ¨¡å‹å¤ªå¤§æ€ä¹ˆåŠï¼Ÿ

**A**: ä½¿ç”¨ `--auto-slice` é€‰é¡¹è‡ªåŠ¨åˆ‡ç‰‡ï¼š

```bash
python tools/model_to_flash.py --input large_model.onnx --output fw.bin --auto-slice
```

å¦‚æœä»ç„¶å¤ªå¤§ï¼Œè€ƒè™‘ï¼š
- ä½¿ç”¨æ¨¡å‹å‰ªæ (`tools/prune_model.py`)
- åº”ç”¨ç¨€ç–åŒ– (`mlir_compiler/sparsity_optimizer.py`)
- ä½¿ç”¨çŸ¥è¯†è’¸é¦å‹ç¼©æ¨¡å‹

### Q2: é‡åŒ–å¯¼è‡´ç²¾åº¦ä¸‹é™ï¼Ÿ

**A**: ä½¿ç”¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT) é¢„å…ˆé€‚åº”é‡åŒ–è¯¯å·®ï¼š

```bash
python mlir_compiler/qat_trainer.py --model model.pth --epochs 20
```

### Q3: å¦‚ä½•éªŒè¯å›ºä»¶æ­£ç¡®æ€§ï¼Ÿ

**A**: ä½¿ç”¨ä»¿çœŸå™¨æµ‹è¯•ï¼š

```bash
python imc22_sdk/python/sim_flash_loader.py --firmware firmware.bin --test
```

---

## å‚è€ƒèµ„æ–™

- [æ¨¡å‹é‡åŒ–åŸç†](../docs/QUANTIZATION.md)
- [CIM SRAM åœ°å€æ˜ å°„](../docs/MEMORY_MAP.md)
- [Flash ç¼–ç¨‹æŒ‡å—](../fpga/docs/FLASH_PROGRAMMING.md)
